\chapter{Data Quality Monitoring (DQM) \label{ch:DQM}}
DQM is for a high-quality data where increasing data size (10 times more Pixel readout channels for Inner Tracker alone during HL-LHC) is creating a huge challenge for shifters to monitor and certify it.  Use of ML techniques in DQM can aid shifters detect data anomalies as detector conditions change. Currently, a shifter assesses data quality by a manual scrutiny - comparing histograms with reference ones, looking for any deviations.
We are exploring ML Playground (MLP), a Django-based framework, to automate DQM. It groups training dataset information, automates ML training, and generates reports based on the performance of ML model.
Fidalgo [31] developed a python code to extract information from Run Registry (RR) on Good/Bad runs. RR along with (1) DQM graphical interface (for 1D and 2D histograms), (2) OMS (Online Monitoring System, for beam conditions like luminosity, pile-up, trigger info) and (3) TkMaps (Tracker Maps for information organized geometrically to visualize issues and data trends in detector components) provides input to the ML framework. Fig. 6 shows a cronjob script by Fidalgo to execute DAS queries and copy files to any area with password-less certificate.


There are two future tasks: Task 1 requires upload of newly collected data from DQMIO (Data Quality Monitoring Input Output) to the ML Playground Graphical User Interface (GUI) and organizing it efficiently for ML use. It requires (1) developing scripts to upload DQMIO files to the dedicated EOS space (EOS provides fast and reliable multi-PB disk-only storage) and track the "health" of the files uploaded, (2)  monitor the file system at regular intervals and discover newly uploaded files, (3) run the MLP parser on the new files, (4) implement robust checks of the files already present in the EOS space and attempt to copy over only newly added files to the list and (5) implement a method that allows for files that are already found in the EOS to be forcibly updated or overwritten at the request of a user, if needed and (6) add logging functionality for detailed bookkeeping in case scripts involved fail. This entails use of DAS (Data Aggregation System) query for files in datasets of interest, iterate through the content generated and copy it to EOS where all DQMIO files are stored for the GUI.
Task 2 will develop a Reference Run Ranking (RRR) tool based on the files provided. When a target run (i.e., a run that the shift leader needs to find a reference run for, during Data Certification (DC) and a set of prior candidate runs are provided to RRR, it will rank these candidate runs from best to worst based on their suitability as reference runs. This should ease the reference run selection and, by identifying high-quality runs, serve as a resource for shifters to find robust training datasets for ML models accessible through the MLP. A reference run should meet two criteria: (1) contain a high amount of LumiSections (LS) to ensure statistical robustness and (2) the runâ€™s data-taking conditions should be as similar as possible to those of the target run.
Past efforts failed to consistently place the reference run at the top of the list of candidates due to two main problems: firstly, the parameters characterizing each run were not normalized, leading to an artificial inflation of the importance of some parameters over others and secondly, the weights assigned to each parameter were not systematically determined but were instead based on expert opinion and experience. To mitigate these issues, feature vectors would be constructed for both target and candidate runs using (1) Run level features like initial luminosity, end luminosity, change in luminosity, delivered luminosity, start time of the run, and run number and (2) LS level features like average and standard deviation of initial luminosity, average and standard deviation of the end luminosity, average and standard deviation of pileup. Weighted Euclidean distance between feature vectors of the candidate runs and the target run would be computed to gauge similarity between them. A systematic approach would be implemented to determine the appropriate weights for this metric, that will include the normalization and possible standardization of features to ensure that they are comparable irrespective of the statistical approach chosen.
